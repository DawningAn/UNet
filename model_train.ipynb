{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 训练unet模型\n",
    "# 1.搭建unet模型\n",
    "# 2.自定义loss 函数\n",
    "# 3.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, where='train', seq=None):  # 根据需要是否做数据增强（seq）\n",
    "        # 获取npy文件\n",
    "        # 图片列表\n",
    "        self.img_list = glob.glob('processed/{}/*/img_*'.format(where))\n",
    "        # 数据增强的处理\n",
    "        self.seq = seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取具体某一个数据\n",
    "        img_file = self.img_list[idx]\n",
    "        # 获取标注文件名\n",
    "        mask_file = img_file.replace('img', 'label')\n",
    "\n",
    "        # 加载\n",
    "        img = np.load(img_file)\n",
    "        mask = np.load(mask_file)\n",
    "\n",
    "        # 数据增强\n",
    "        if self.seq:\n",
    "            segmap = SegmentationMapsOnImage(mask, mask.shape)\n",
    "            img, mask = self.seq(image=img, segmentation_maps=segmap)\n",
    "            # 获取数组内容\n",
    "            mask = mask.get_arr()\n",
    "        # 扩张维度变为张量\n",
    "        return np.expand_dims(img, 0), np.expand_dims(mask, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 数据增强处理流程\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(scale=(0.8, 1.2),  # 缩放\n",
    "               rotate=(-45, 45)),  # 旋转\n",
    "    iaa.ElasticTransformation()  # 变换\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 使用dataloader加载\n",
    "batch_size = 12\n",
    "num_workers = 0\n",
    "\n",
    "train_dataset = SegmentDataset('train', seq)\n",
    "test_dataset = SegmentDataset('test', None)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 导入UNet模型\n",
    "# 两次卷积操作\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.step = torch.nn.Sequential(\n",
    "            # 第一次卷积 (不改变大小，只改变输出通道数)\n",
    "            torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            #ReLU\n",
    "            torch.nn.ReLU(),\n",
    "            # 第二次卷积 (不改变大小，不改变输出通道数)\n",
    "            torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            #ReLU\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.step(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 定义网络架构 (下采样：最大池化 上采样：双线性插值 特征融合：)\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义左半部分\n",
    "        self.layer1 = ConvBlock(1, 64)\n",
    "        self.layer2 = ConvBlock(64, 128)\n",
    "        self.layer3 = ConvBlock(128, 256)\n",
    "        self.layer4 = ConvBlock(256, 512)\n",
    "\n",
    "        # 定义右半部分\n",
    "        self.layer5 = ConvBlock(256 + 512, 256)\n",
    "        self.layer6 = ConvBlock(128 + 256, 128)\n",
    "        self.layer7 = ConvBlock(64 + 128, 64)\n",
    "\n",
    "        # 最后一个卷积\n",
    "        self.layer8 = torch.nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        #池化\n",
    "        self.Maxpool = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # 上采样 -- scale_factor:放大倍数\n",
    "        self.UpSample = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "        #sigmoid\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对输入数据 x进行处理 (下采样)\n",
    "        # input:(1*256*256) output:(64*256*256)\n",
    "        x1 = self.layer1(x)\n",
    "        # 池化\n",
    "        # input:(64*256*256) output:(64*128*128)\n",
    "        x1_mp = self.Maxpool(x1)\n",
    "\n",
    "        # input:(64*128*128) output:(128*128*128)\n",
    "        x2 = self.layer2(x1_mp)\n",
    "        # input:(128*128*128) output:(128*64*64)\n",
    "        x2_mp = self.Maxpool(x2)\n",
    "\n",
    "        # input:(128*64*64) output:(256*64*64)\n",
    "        x3 = self.layer3(x2_mp)\n",
    "        # input:(256*64*64) output:(256*32*32)\n",
    "        x3_mp = self.Maxpool(x3)\n",
    "\n",
    "        # input:(256*32*32) output:(512*32*32)\n",
    "        x4 = self.layer4(x3_mp)\n",
    "\n",
    "        # 上采样部分\n",
    "        # input:(512*32*32) output:(512*64*64)\n",
    "        x5 = self.UpSample(x4)\n",
    "        # 特征拼接 x3 和 x5\n",
    "        x5 = torch.cat([x5, x3], dim=1)  # 在通道维度上拼接   output:(768*64*64)\n",
    "        # 卷积 intput:(768*64*64)  output:(256*64*64)\n",
    "        x5 = self.layer5(x5)\n",
    "\n",
    "        # intput:(256*64*64)  output:(256*128*128)\n",
    "        x6 = self.UpSample(x5)\n",
    "        # 拼接 在通道维度上拼接 output:(384,128,128)\n",
    "        x6 = torch.cat([x6, x2], dim=1)\n",
    "        # 卷积 intput:(384*128*128)  output:(128*128*128)\n",
    "        x6 = self.layer6(x6)\n",
    "\n",
    "        # intput:(128*128*128) output:(128*256*256)\n",
    "        x7 = self.UpSample(x6)\n",
    "        # 拼接 在通道维度上拼接 output:(64+ 128*256*256)\n",
    "        x7 = torch.cat([x7, x1], dim=1)\n",
    "        # 卷积 input:(192*256*256) output:(64*256*256)\n",
    "        x7 = self.layer7(x7)\n",
    "\n",
    "        # 最后一次卷积\n",
    "        # input:(64*256*256) output:(1*256*256)\n",
    "        x8 = self.layer8(x7)\n",
    "\n",
    "        #sigmoid\n",
    "        x9 = self.sigmoid(x8)\n",
    "\n",
    "        return x9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = UNet().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             640\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "         ConvBlock-5         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-6         [-1, 64, 128, 128]               0\n",
      "            Conv2d-7        [-1, 128, 128, 128]          73,856\n",
      "              ReLU-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9        [-1, 128, 128, 128]         147,584\n",
      "             ReLU-10        [-1, 128, 128, 128]               0\n",
      "        ConvBlock-11        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-12          [-1, 128, 64, 64]               0\n",
      "           Conv2d-13          [-1, 256, 64, 64]         295,168\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-16          [-1, 256, 64, 64]               0\n",
      "        ConvBlock-17          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19          [-1, 512, 32, 32]       1,180,160\n",
      "             ReLU-20          [-1, 512, 32, 32]               0\n",
      "           Conv2d-21          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-22          [-1, 512, 32, 32]               0\n",
      "        ConvBlock-23          [-1, 512, 32, 32]               0\n",
      "         Upsample-24          [-1, 512, 64, 64]               0\n",
      "           Conv2d-25          [-1, 256, 64, 64]       1,769,728\n",
      "             ReLU-26          [-1, 256, 64, 64]               0\n",
      "           Conv2d-27          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-28          [-1, 256, 64, 64]               0\n",
      "        ConvBlock-29          [-1, 256, 64, 64]               0\n",
      "         Upsample-30        [-1, 256, 128, 128]               0\n",
      "           Conv2d-31        [-1, 128, 128, 128]         442,496\n",
      "             ReLU-32        [-1, 128, 128, 128]               0\n",
      "           Conv2d-33        [-1, 128, 128, 128]         147,584\n",
      "             ReLU-34        [-1, 128, 128, 128]               0\n",
      "        ConvBlock-35        [-1, 128, 128, 128]               0\n",
      "         Upsample-36        [-1, 128, 256, 256]               0\n",
      "           Conv2d-37         [-1, 64, 256, 256]         110,656\n",
      "             ReLU-38         [-1, 64, 256, 256]               0\n",
      "           Conv2d-39         [-1, 64, 256, 256]          36,928\n",
      "             ReLU-40         [-1, 64, 256, 256]               0\n",
      "        ConvBlock-41         [-1, 64, 256, 256]               0\n",
      "           Conv2d-42          [-1, 1, 256, 256]              65\n",
      "          Sigmoid-43          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 7,781,761\n",
      "Trainable params: 7,781,761\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 707.00\n",
      "Params size (MB): 29.69\n",
      "Estimated Total Size (MB): 736.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 256, 256))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 模拟输入输出\n",
    "random_input = torch.randn((1, 1, 256, 256)).to(device)  # 第一个1代表当前批次数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "output = model(random_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0.5272, 0.5278, 0.5274,  ..., 0.5283, 0.5276, 0.5276],\n          [0.5258, 0.5260, 0.5273,  ..., 0.5282, 0.5286, 0.5284],\n          [0.5259, 0.5280, 0.5285,  ..., 0.5283, 0.5267, 0.5285],\n          ...,\n          [0.5263, 0.5283, 0.5263,  ..., 0.5279, 0.5268, 0.5287],\n          [0.5272, 0.5279, 0.5276,  ..., 0.5262, 0.5279, 0.5293],\n          [0.5270, 0.5271, 0.5273,  ..., 0.5277, 0.5280, 0.5281]]]],\n       device='cuda:0', grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 训练\n",
    "# 定义Dice loss系数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Diceloss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        # 把数值拉成向量\n",
    "        pred = torch.flatten(pred)\n",
    "        mask = torch.flatten(mask)\n",
    "\n",
    "        # 交集面积 （两个向量相乘的结果大致为其相交面积）\n",
    "        overlap = (pred * mask).sum()\n",
    "        # 分母\n",
    "        denum = pred.sum() + mask.sum() + 1e-8\n",
    "\n",
    "        dice = (2 * overlap) / denum\n",
    "\n",
    "        return 1 - dice\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_fn = Diceloss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 优化去\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 动态减少学习率\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler =  ReduceLROnPlateau(optimizer,'min')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='./log')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 计算测试集loss\n",
    "def check_test_loss(loader,model):\n",
    "    # 记录loss\n",
    "    loss =0\n",
    "    # 不记录梯度\n",
    "    with torch.no_grad():\n",
    "        # 遍历测试数据\n",
    "        for i, (x,y) in enumerate(loader):\n",
    "            # 获取图像\n",
    "            x = x.to(device,dtype = torch.float32)\n",
    "            # 获取标注\n",
    "            y = y.to(device,dtype = torch.float32)\n",
    "\n",
    "            # 获取预测值\n",
    "            y_pred = model(x)\n",
    "            loss_batch = loss_fn(y_pred,y)\n",
    "\n",
    "            loss += loss_batch\n",
    "    loss = loss / len(loader)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4848)\n",
      "tensor(0.5506)\n",
      "tensor(0.6036)\n",
      "tensor(0.5632)\n",
      "tensor(0.8292)\n",
      "tensor(0.6965)\n",
      "tensor(0.7852)\n",
      "tensor(0.5116)\n",
      "tensor(0.5295)\n",
      "tensor(0.4348)\n",
      "tensor(0.4554)\n",
      "tensor(0.4753)\n",
      "tensor(0.7221)\n",
      "tensor(0.7945)\n",
      "tensor(0.4631)\n",
      "tensor(0.6561)\n",
      "tensor(0.5972)\n",
      "tensor(0.4419)\n",
      "tensor(0.4936)\n",
      "tensor(0.6998)\n",
      "tensor(0.7411)\n",
      "tensor(0.4426)\n",
      "tensor(0.6162)\n",
      "tensor(0.5234)\n",
      "tensor(0.4936)\n",
      "tensor(0.4858)\n",
      "tensor(0.5425)\n",
      "tensor(0.4645)\n",
      "tensor(0.8117)\n",
      "tensor(0.5412)\n",
      "tensor(0.6749)\n",
      "tensor(0.8836)\n",
      "tensor(0.6422)\n",
      "tensor(0.6120)\n",
      "tensor(0.7668)\n",
      "tensor(0.6734)\n",
      "tensor(0.6571)\n",
      "tensor(0.6429)\n",
      "tensor(0.6776)\n",
      "tensor(0.6695)\n",
      "tensor(0.5692)\n",
      "tensor(0.4809)\n",
      "tensor(0.7584)\n",
      "tensor(0.8257)\n",
      "tensor(0.4827)\n",
      "tensor(0.4992)\n",
      "tensor(0.5700)\n",
      "tensor(0.6650)\n",
      "tensor(0.5146)\n",
      "tensor(0.6895)\n",
      "tensor(0.4380)\n",
      "tensor(0.5415)\n",
      "tensor(0.6955)\n",
      "tensor(0.4438)\n",
      "tensor(0.6065)\n",
      "tensor(0.4873)\n",
      "tensor(0.5547)\n",
      "tensor(0.5084)\n",
      "tensor(0.5525)\n",
      "tensor(0.3543)\n",
      "tensor(0.6670)\n",
      "tensor(0.6543)\n",
      "tensor(0.4019)\n",
      "tensor(0.5134)\n",
      "tensor(0.4351)\n",
      "tensor(0.5240)\n",
      "tensor(0.5064)\n",
      "tensor(0.5832)\n",
      "tensor(0.5925)\n",
      "tensor(0.6033)\n",
      "tensor(0.5823)\n",
      "tensor(0.4928)\n",
      "tensor(0.4953)\n",
      "tensor(0.6130)\n",
      "tensor(0.4559)\n",
      "tensor(0.5776)\n",
      "tensor(0.4414)\n",
      "tensor(0.4047)\n",
      "tensor(0.6616)\n",
      "tensor(0.4044)\n",
      "tensor(0.7162)\n",
      "tensor(0.6992)\n",
      "tensor(0.5162)\n",
      "tensor(0.6424)\n",
      "tensor(0.6039)\n",
      "tensor(0.6118)\n",
      "tensor(0.5115)\n",
      "tensor(0.6922)\n",
      "tensor(0.5587)\n",
      "tensor(0.4154)\n",
      "tensor(0.6183)\n",
      "tensor(0.5328)\n",
      "tensor(0.4852)\n",
      "tensor(0.6096)\n",
      "tensor(0.4381)\n",
      "tensor(0.5946)\n",
      "tensor(0.4416)\n",
      "tensor(0.4827)\n",
      "tensor(0.5589)\n",
      "tensor(0.7183)\n",
      "tensor(0.7348)\n",
      "tensor(0.5395)\n",
      "tensor(0.5253)\n",
      "tensor(0.4674)\n",
      "tensor(0.5655)\n",
      "tensor(0.6610)\n",
      "tensor(0.7537)\n",
      "tensor(0.4851)\n",
      "tensor(0.6595)\n",
      "tensor(0.6947)\n",
      "tensor(0.4490)\n",
      "tensor(0.5448)\n",
      "tensor(0.4380)\n",
      "tensor(0.4172)\n",
      "tensor(0.4708)\n",
      "tensor(0.5328)\n",
      "tensor(0.5680)\n",
      "tensor(0.5107)\n",
      "tensor(0.6981)\n",
      "tensor(0.5693)\n",
      "tensor(0.5074)\n",
      "tensor(0.5264)\n",
      "tensor(0.5644)\n",
      "tensor(0.5542)\n",
      "tensor(0.4310)\n",
      "tensor(0.6886)\n",
      "tensor(0.5447)\n",
      "tensor(0.5191)\n",
      "tensor(0.6421)\n",
      "tensor(0.5048)\n",
      "tensor(0.4943)\n",
      "tensor(0.4831)\n",
      "tensor(0.4953)\n",
      "tensor(0.5100)\n",
      "tensor(0.6101)\n",
      "tensor(0.3928)\n",
      "tensor(0.5572)\n",
      "tensor(0.4026)\n",
      "tensor(0.4024)\n",
      "tensor(0.5122)\n",
      "tensor(0.6988)\n",
      "tensor(0.4578)\n",
      "tensor(0.3725)\n",
      "tensor(0.5368)\n",
      "tensor(0.4197)\n",
      "tensor(0.4065)\n",
      "tensor(0.3610)\n",
      "tensor(0.5048)\n",
      "tensor(0.5497)\n",
      "tensor(0.4051)\n",
      "tensor(0.5038)\n",
      "tensor(0.4441)\n",
      "tensor(0.5358)\n",
      "tensor(0.6355)\n",
      "tensor(0.4940)\n",
      "tensor(0.5195)\n",
      "tensor(0.5979)\n",
      "tensor(0.5028)\n",
      "tensor(0.6047)\n",
      "tensor(0.5879)\n",
      "tensor(0.5337)\n",
      "第0个EPOCH达到最低loss\n",
      "第0个Epoch的训练时间107.12588882446289s, TrainLoss:0.5586711764335632, TestLoss:0.6760475635528564\n",
      "tensor(0.3226)\n",
      "tensor(0.4849)\n",
      "tensor(0.5724)\n",
      "tensor(0.4369)\n",
      "tensor(0.4133)\n",
      "tensor(0.5132)\n",
      "tensor(0.5247)\n",
      "tensor(0.5588)\n",
      "tensor(0.4504)\n",
      "tensor(0.4415)\n",
      "tensor(0.6329)\n",
      "tensor(0.5281)\n",
      "tensor(0.5291)\n",
      "tensor(0.5440)\n",
      "tensor(0.6396)\n",
      "tensor(0.3727)\n",
      "tensor(0.4547)\n",
      "tensor(0.6598)\n",
      "tensor(0.5182)\n",
      "tensor(0.5865)\n",
      "tensor(0.4074)\n",
      "tensor(0.4548)\n",
      "tensor(0.4416)\n",
      "tensor(0.5208)\n",
      "tensor(0.7214)\n",
      "tensor(0.4518)\n",
      "tensor(0.5290)\n",
      "tensor(0.5504)\n",
      "tensor(0.5437)\n",
      "tensor(0.6133)\n",
      "tensor(0.5620)\n",
      "tensor(0.5240)\n",
      "tensor(0.4446)\n",
      "tensor(0.4917)\n",
      "tensor(0.5201)\n",
      "tensor(0.6661)\n",
      "tensor(0.4898)\n",
      "tensor(0.5877)\n",
      "tensor(0.7378)\n",
      "tensor(0.4626)\n",
      "tensor(0.4421)\n",
      "tensor(0.4832)\n",
      "tensor(0.6279)\n",
      "tensor(0.5126)\n",
      "tensor(0.4675)\n",
      "tensor(0.4537)\n",
      "tensor(0.6370)\n",
      "tensor(0.5853)\n",
      "tensor(0.4670)\n",
      "tensor(0.6290)\n",
      "tensor(0.5924)\n",
      "tensor(0.4468)\n",
      "tensor(0.7194)\n",
      "tensor(0.5038)\n",
      "tensor(0.3886)\n",
      "tensor(0.5291)\n",
      "tensor(0.3757)\n",
      "tensor(0.4543)\n",
      "tensor(0.4209)\n",
      "tensor(0.5101)\n",
      "tensor(0.6757)\n",
      "tensor(0.4159)\n",
      "tensor(0.3678)\n",
      "tensor(0.5078)\n",
      "tensor(0.5323)\n",
      "tensor(0.5685)\n",
      "tensor(0.5464)\n",
      "tensor(0.5128)\n",
      "tensor(0.4851)\n",
      "tensor(0.4330)\n",
      "tensor(0.4872)\n",
      "tensor(0.6256)\n",
      "tensor(0.4346)\n",
      "tensor(0.4772)\n",
      "tensor(0.4407)\n",
      "tensor(0.5170)\n",
      "tensor(0.7758)\n",
      "tensor(0.4318)\n",
      "tensor(0.6043)\n",
      "tensor(0.5432)\n",
      "tensor(0.3412)\n",
      "tensor(0.5534)\n",
      "tensor(0.6865)\n",
      "tensor(0.6433)\n",
      "tensor(0.5787)\n",
      "tensor(0.6026)\n",
      "tensor(0.5660)\n",
      "tensor(0.5190)\n",
      "tensor(0.3857)\n",
      "tensor(0.5132)\n",
      "tensor(0.4343)\n",
      "tensor(0.3539)\n",
      "tensor(0.6734)\n",
      "tensor(0.5364)\n",
      "tensor(0.3774)\n",
      "tensor(0.4754)\n",
      "tensor(0.3758)\n",
      "tensor(0.4900)\n",
      "tensor(0.4043)\n",
      "tensor(0.5662)\n",
      "tensor(0.4965)\n",
      "tensor(0.4136)\n",
      "tensor(0.5035)\n",
      "tensor(0.4149)\n",
      "tensor(0.5695)\n",
      "tensor(0.4247)\n",
      "tensor(0.5568)\n",
      "tensor(0.4660)\n",
      "tensor(0.5354)\n",
      "tensor(0.5837)\n",
      "tensor(0.3657)\n",
      "tensor(0.5242)\n",
      "tensor(0.4794)\n",
      "tensor(0.6957)\n",
      "tensor(0.6838)\n",
      "tensor(0.4119)\n",
      "tensor(0.3847)\n",
      "tensor(0.3977)\n",
      "tensor(0.4465)\n",
      "tensor(0.5453)\n",
      "tensor(0.4983)\n",
      "tensor(0.4976)\n",
      "tensor(0.3580)\n",
      "tensor(0.4149)\n",
      "tensor(0.5463)\n",
      "tensor(0.4598)\n",
      "tensor(0.5656)\n",
      "tensor(0.4519)\n",
      "tensor(0.6705)\n",
      "tensor(0.3855)\n",
      "tensor(0.5705)\n",
      "tensor(0.3398)\n",
      "tensor(0.4390)\n",
      "tensor(0.4668)\n",
      "tensor(0.3899)\n",
      "tensor(0.4204)\n",
      "tensor(0.5053)\n",
      "tensor(0.4160)\n",
      "tensor(0.3711)\n",
      "tensor(0.4438)\n",
      "tensor(0.5161)\n",
      "tensor(0.6043)\n",
      "tensor(0.4653)\n",
      "tensor(0.3750)\n",
      "tensor(0.5116)\n",
      "tensor(0.5367)\n",
      "tensor(0.3976)\n",
      "tensor(0.4408)\n",
      "tensor(0.6271)\n",
      "tensor(0.4193)\n",
      "tensor(0.5691)\n",
      "tensor(0.4686)\n",
      "tensor(0.3878)\n",
      "tensor(0.5764)\n",
      "tensor(0.6025)\n",
      "tensor(0.4991)\n",
      "tensor(0.5282)\n",
      "tensor(0.5300)\n",
      "tensor(0.7157)\n",
      "tensor(0.4805)\n",
      "tensor(0.4578)\n",
      "第1个Epoch的训练时间99.30427384376526s, TrainLoss:0.5057811141014099, TestLoss:0.760613203048706\n",
      "tensor(0.8069)\n",
      "tensor(0.5934)\n",
      "tensor(0.4872)\n",
      "tensor(0.5467)\n",
      "tensor(0.3923)\n",
      "tensor(0.6608)\n",
      "tensor(0.4244)\n",
      "tensor(0.4347)\n",
      "tensor(0.6862)\n",
      "tensor(0.5452)\n",
      "tensor(0.3421)\n",
      "tensor(0.4452)\n",
      "tensor(0.4599)\n",
      "tensor(0.6139)\n",
      "tensor(0.4588)\n",
      "tensor(0.4434)\n",
      "tensor(0.4759)\n",
      "tensor(0.6854)\n",
      "tensor(0.5864)\n",
      "tensor(0.6154)\n",
      "tensor(0.5572)\n",
      "tensor(0.4571)\n",
      "tensor(0.5073)\n",
      "tensor(0.4097)\n",
      "tensor(0.5936)\n",
      "tensor(0.4422)\n",
      "tensor(0.4451)\n",
      "tensor(0.4487)\n",
      "tensor(0.3413)\n",
      "tensor(0.3755)\n",
      "tensor(0.4659)\n",
      "tensor(0.3793)\n",
      "tensor(0.3580)\n",
      "tensor(0.4581)\n",
      "tensor(0.4921)\n",
      "tensor(0.4638)\n",
      "tensor(0.4180)\n",
      "tensor(0.4819)\n",
      "tensor(0.4746)\n",
      "tensor(0.4866)\n",
      "tensor(0.5104)\n",
      "tensor(0.8020)\n",
      "tensor(0.4491)\n",
      "tensor(0.3772)\n",
      "tensor(0.2863)\n",
      "tensor(0.4353)\n",
      "tensor(0.4195)\n",
      "tensor(0.5059)\n",
      "tensor(0.5373)\n",
      "tensor(0.4911)\n",
      "tensor(0.4557)\n",
      "tensor(0.3454)\n",
      "tensor(0.4366)\n",
      "tensor(0.5435)\n",
      "tensor(0.4795)\n",
      "tensor(0.5092)\n",
      "tensor(0.4782)\n",
      "tensor(0.4371)\n",
      "tensor(0.4761)\n",
      "tensor(0.3750)\n",
      "tensor(0.5006)\n",
      "tensor(0.3939)\n",
      "tensor(0.4081)\n",
      "tensor(0.3705)\n",
      "tensor(0.5280)\n",
      "tensor(0.6616)\n",
      "tensor(0.8490)\n",
      "tensor(0.7627)\n",
      "tensor(0.3818)\n",
      "tensor(0.3858)\n",
      "tensor(0.2986)\n",
      "tensor(0.4757)\n",
      "tensor(0.5376)\n",
      "tensor(0.4963)\n",
      "tensor(0.6514)\n",
      "tensor(0.5495)\n",
      "tensor(0.5777)\n",
      "tensor(0.5040)\n",
      "tensor(0.4947)\n",
      "tensor(0.5698)\n",
      "tensor(0.3680)\n",
      "tensor(0.4463)\n",
      "tensor(0.4071)\n",
      "tensor(0.4363)\n",
      "tensor(0.4582)\n",
      "tensor(0.4384)\n",
      "tensor(0.4522)\n",
      "tensor(0.3744)\n",
      "tensor(0.5062)\n",
      "tensor(0.4512)\n",
      "tensor(0.5316)\n",
      "tensor(0.4099)\n",
      "tensor(0.3872)\n",
      "tensor(0.5116)\n",
      "tensor(0.4061)\n",
      "tensor(0.4218)\n",
      "tensor(0.4449)\n",
      "tensor(0.3315)\n",
      "tensor(0.5017)\n",
      "tensor(0.3336)\n",
      "tensor(0.6122)\n",
      "tensor(0.4797)\n",
      "tensor(0.4649)\n",
      "tensor(0.4090)\n",
      "tensor(0.4702)\n",
      "tensor(0.3436)\n",
      "tensor(0.4695)\n",
      "tensor(0.2976)\n",
      "tensor(0.4281)\n",
      "tensor(0.2937)\n",
      "tensor(0.3411)\n",
      "tensor(0.5297)\n",
      "tensor(0.8792)\n",
      "tensor(0.2462)\n",
      "tensor(0.5773)\n",
      "tensor(0.3934)\n",
      "tensor(0.5673)\n",
      "tensor(0.4500)\n",
      "tensor(0.3445)\n",
      "tensor(0.6673)\n",
      "tensor(0.7045)\n",
      "tensor(0.5613)\n",
      "tensor(0.6035)\n",
      "tensor(0.4186)\n",
      "tensor(0.4764)\n",
      "tensor(0.4029)\n",
      "tensor(0.4160)\n",
      "tensor(0.3758)\n",
      "tensor(0.6160)\n",
      "tensor(0.4526)\n",
      "tensor(0.4510)\n",
      "tensor(0.3994)\n",
      "tensor(0.4078)\n",
      "tensor(0.4936)\n",
      "tensor(0.5136)\n",
      "tensor(0.3591)\n",
      "tensor(0.3966)\n",
      "tensor(0.4655)\n",
      "tensor(0.3810)\n",
      "tensor(0.7733)\n",
      "tensor(0.4310)\n",
      "tensor(0.4632)\n",
      "tensor(0.4958)\n",
      "tensor(0.4273)\n",
      "tensor(0.5719)\n",
      "tensor(0.3083)\n",
      "tensor(0.3489)\n",
      "tensor(0.6058)\n",
      "tensor(0.4698)\n",
      "tensor(0.4489)\n",
      "tensor(0.4186)\n",
      "tensor(0.3992)\n",
      "tensor(0.3516)\n",
      "tensor(0.3690)\n",
      "tensor(0.5244)\n",
      "tensor(0.4013)\n",
      "tensor(0.4009)\n",
      "tensor(0.3640)\n",
      "tensor(0.5067)\n",
      "tensor(0.4608)\n",
      "tensor(0.5120)\n",
      "第2个EPOCH达到最低loss\n",
      "第2个Epoch的训练时间103.20315098762512s, TrainLoss:0.47415056824684143, TestLoss:0.6514414548873901\n",
      "tensor(0.3781)\n",
      "tensor(0.5899)\n",
      "tensor(0.4901)\n",
      "tensor(0.5604)\n",
      "tensor(0.3633)\n",
      "tensor(0.6059)\n",
      "tensor(0.3264)\n",
      "tensor(0.3708)\n",
      "tensor(0.3963)\n",
      "tensor(0.6724)\n",
      "tensor(0.3881)\n",
      "tensor(0.4474)\n",
      "tensor(0.3348)\n",
      "tensor(0.4065)\n",
      "tensor(0.6455)\n",
      "tensor(0.4079)\n",
      "tensor(0.3724)\n",
      "tensor(0.5628)\n",
      "tensor(0.4147)\n",
      "tensor(0.4772)\n",
      "tensor(0.3227)\n",
      "tensor(0.5673)\n",
      "tensor(0.4114)\n",
      "tensor(0.7118)\n",
      "tensor(0.5524)\n",
      "tensor(0.4945)\n",
      "tensor(0.4867)\n",
      "tensor(0.4997)\n",
      "tensor(0.5053)\n",
      "tensor(0.5708)\n",
      "tensor(0.4877)\n",
      "tensor(0.5252)\n",
      "tensor(0.5821)\n",
      "tensor(0.3822)\n",
      "tensor(0.3000)\n",
      "tensor(0.3927)\n",
      "tensor(0.4567)\n",
      "tensor(0.3463)\n",
      "tensor(0.5856)\n",
      "tensor(0.4431)\n",
      "tensor(0.3374)\n",
      "tensor(0.3448)\n",
      "tensor(0.6971)\n",
      "tensor(0.5180)\n",
      "tensor(0.4290)\n",
      "tensor(0.3884)\n",
      "tensor(0.5137)\n",
      "tensor(0.5425)\n",
      "tensor(0.3806)\n",
      "tensor(0.4863)\n",
      "tensor(0.5993)\n",
      "tensor(0.4132)\n",
      "tensor(0.3355)\n",
      "tensor(0.3807)\n",
      "tensor(0.4559)\n",
      "tensor(0.7865)\n",
      "tensor(0.5336)\n",
      "tensor(0.5565)\n",
      "tensor(0.5478)\n",
      "tensor(0.3841)\n",
      "tensor(0.8233)\n",
      "tensor(0.4243)\n",
      "tensor(0.4446)\n",
      "tensor(0.4320)\n",
      "tensor(0.5492)\n",
      "tensor(0.3631)\n",
      "tensor(0.5212)\n",
      "tensor(0.4757)\n",
      "tensor(0.4347)\n",
      "tensor(0.4375)\n",
      "tensor(0.2959)\n",
      "tensor(0.4193)\n",
      "tensor(0.5387)\n",
      "tensor(0.4517)\n",
      "tensor(0.5316)\n",
      "tensor(0.3937)\n",
      "tensor(0.3611)\n",
      "tensor(0.3916)\n",
      "tensor(0.3818)\n",
      "tensor(0.5248)\n",
      "tensor(0.6005)\n",
      "tensor(0.5646)\n",
      "tensor(0.4614)\n",
      "tensor(0.4584)\n",
      "tensor(0.3258)\n",
      "tensor(0.3466)\n",
      "tensor(0.5166)\n",
      "tensor(0.5184)\n",
      "tensor(0.5244)\n",
      "tensor(0.5394)\n",
      "tensor(0.4473)\n",
      "tensor(0.3966)\n",
      "tensor(0.4112)\n",
      "tensor(0.3499)\n",
      "tensor(0.4297)\n",
      "tensor(0.4341)\n",
      "tensor(0.5310)\n",
      "tensor(0.4438)\n",
      "tensor(0.4726)\n",
      "tensor(0.5984)\n",
      "tensor(0.4091)\n",
      "tensor(0.3692)\n",
      "tensor(0.6255)\n",
      "tensor(0.4599)\n",
      "tensor(0.4269)\n",
      "tensor(0.3256)\n",
      "tensor(0.4856)\n",
      "tensor(0.3021)\n",
      "tensor(0.6918)\n",
      "tensor(0.3205)\n",
      "tensor(0.3559)\n",
      "tensor(0.3006)\n",
      "tensor(0.3772)\n",
      "tensor(0.3875)\n",
      "tensor(0.2971)\n",
      "tensor(0.4178)\n",
      "tensor(0.5492)\n",
      "tensor(0.3937)\n",
      "tensor(0.3649)\n",
      "tensor(0.3561)\n",
      "tensor(0.4791)\n",
      "tensor(0.4310)\n",
      "tensor(0.4369)\n",
      "tensor(0.3502)\n",
      "tensor(0.6411)\n",
      "tensor(0.2665)\n",
      "tensor(0.4460)\n",
      "tensor(0.4218)\n",
      "tensor(0.3714)\n",
      "tensor(0.6255)\n",
      "tensor(0.2771)\n",
      "tensor(0.3174)\n",
      "tensor(0.4375)\n",
      "tensor(0.5889)\n",
      "tensor(0.5227)\n",
      "tensor(0.4249)\n",
      "tensor(0.4000)\n",
      "tensor(0.3686)\n",
      "tensor(0.2896)\n",
      "tensor(0.3372)\n",
      "tensor(0.3960)\n",
      "tensor(0.3702)\n",
      "tensor(0.4207)\n",
      "tensor(0.6017)\n",
      "tensor(0.4168)\n",
      "tensor(0.2790)\n",
      "tensor(0.4384)\n",
      "tensor(0.3826)\n",
      "tensor(0.3019)\n",
      "tensor(0.3486)\n",
      "tensor(0.4421)\n",
      "tensor(0.5486)\n",
      "tensor(0.5371)\n",
      "tensor(0.3398)\n",
      "tensor(0.4431)\n",
      "tensor(0.3443)\n",
      "tensor(0.5178)\n",
      "tensor(0.4374)\n",
      "tensor(0.4732)\n",
      "tensor(0.4573)\n",
      "tensor(0.4203)\n",
      "第3个EPOCH达到最低loss\n",
      "第3个Epoch的训练时间103.2987859249115s, TrainLoss:0.45092612504959106, TestLoss:0.6481534242630005\n",
      "tensor(0.6233)\n",
      "tensor(0.3737)\n",
      "tensor(0.3777)\n",
      "tensor(0.4054)\n",
      "tensor(0.6424)\n",
      "tensor(0.5054)\n",
      "tensor(0.5452)\n",
      "tensor(0.3967)\n",
      "tensor(0.4216)\n",
      "tensor(0.2566)\n",
      "tensor(0.4502)\n",
      "tensor(0.4931)\n",
      "tensor(0.5309)\n",
      "tensor(0.4195)\n",
      "tensor(0.3984)\n",
      "tensor(0.2469)\n",
      "tensor(0.4833)\n",
      "tensor(0.4220)\n",
      "tensor(0.3702)\n",
      "tensor(0.4318)\n",
      "tensor(0.4530)\n",
      "tensor(0.3882)\n",
      "tensor(0.5608)\n",
      "tensor(0.8746)\n",
      "tensor(0.2552)\n",
      "tensor(0.3535)\n",
      "tensor(0.2424)\n",
      "tensor(0.5759)\n",
      "tensor(0.4795)\n",
      "tensor(0.5689)\n",
      "tensor(0.2803)\n",
      "tensor(0.3891)\n",
      "tensor(0.4322)\n",
      "tensor(0.6195)\n",
      "tensor(0.5432)\n",
      "tensor(0.3427)\n",
      "tensor(0.3675)\n",
      "tensor(0.4037)\n",
      "tensor(0.4112)\n",
      "tensor(0.4130)\n",
      "tensor(0.3546)\n",
      "tensor(0.5089)\n",
      "tensor(0.3327)\n",
      "tensor(0.5816)\n",
      "tensor(0.4114)\n",
      "tensor(0.3281)\n",
      "tensor(0.4257)\n",
      "tensor(0.4485)\n",
      "tensor(0.3574)\n",
      "tensor(0.6396)\n",
      "tensor(0.3584)\n",
      "tensor(0.4391)\n",
      "tensor(0.7644)\n",
      "tensor(0.5003)\n",
      "tensor(0.2849)\n",
      "tensor(0.3879)\n",
      "tensor(0.2459)\n",
      "tensor(0.4237)\n",
      "tensor(0.3782)\n",
      "tensor(0.3609)\n",
      "tensor(0.7330)\n",
      "tensor(0.3145)\n",
      "tensor(0.2935)\n",
      "tensor(0.2707)\n",
      "tensor(0.2583)\n",
      "tensor(0.3085)\n",
      "tensor(0.3487)\n",
      "tensor(0.4019)\n",
      "tensor(0.4137)\n",
      "tensor(0.3435)\n",
      "tensor(0.4017)\n",
      "tensor(0.4559)\n",
      "tensor(0.3287)\n",
      "tensor(0.4280)\n",
      "tensor(0.2924)\n",
      "tensor(0.3771)\n",
      "tensor(0.2334)\n",
      "tensor(0.3160)\n",
      "tensor(0.2430)\n",
      "tensor(0.4064)\n",
      "tensor(0.3049)\n",
      "tensor(0.3548)\n",
      "tensor(0.4644)\n",
      "tensor(0.4288)\n",
      "tensor(0.5434)\n",
      "tensor(0.3449)\n",
      "tensor(0.2551)\n",
      "tensor(0.3393)\n",
      "tensor(0.4037)\n",
      "tensor(0.2456)\n",
      "tensor(0.4964)\n",
      "tensor(0.4986)\n",
      "tensor(0.2333)\n",
      "tensor(0.4217)\n",
      "tensor(0.5613)\n",
      "tensor(0.6748)\n",
      "tensor(0.3300)\n",
      "tensor(0.2371)\n",
      "tensor(0.3715)\n",
      "tensor(0.4496)\n",
      "tensor(0.5063)\n",
      "tensor(0.4018)\n",
      "tensor(0.3427)\n",
      "tensor(0.4134)\n",
      "tensor(0.3893)\n",
      "tensor(0.4431)\n",
      "tensor(0.3940)\n",
      "tensor(0.3934)\n",
      "tensor(0.3939)\n",
      "tensor(0.3731)\n",
      "tensor(0.3219)\n",
      "tensor(0.3416)\n",
      "tensor(0.2922)\n",
      "tensor(0.2552)\n",
      "tensor(0.3514)\n",
      "tensor(0.3752)\n",
      "tensor(0.3808)\n",
      "tensor(0.3549)\n",
      "tensor(0.4422)\n",
      "tensor(0.3182)\n",
      "tensor(0.5165)\n",
      "tensor(0.5497)\n",
      "tensor(0.4028)\n",
      "tensor(0.6080)\n",
      "tensor(0.5147)\n",
      "tensor(0.3974)\n",
      "tensor(0.5247)\n",
      "tensor(0.4176)\n",
      "tensor(0.3515)\n",
      "tensor(0.4048)\n",
      "tensor(0.4323)\n",
      "tensor(0.4664)\n",
      "tensor(0.7009)\n",
      "tensor(0.3105)\n",
      "tensor(0.3061)\n",
      "tensor(0.5002)\n",
      "tensor(0.6318)\n",
      "tensor(0.3119)\n",
      "tensor(0.3564)\n",
      "tensor(0.4013)\n",
      "tensor(0.3287)\n",
      "tensor(0.3233)\n",
      "tensor(0.4346)\n",
      "tensor(0.2664)\n",
      "tensor(0.4299)\n",
      "tensor(0.4964)\n",
      "tensor(0.4849)\n",
      "tensor(0.3070)\n",
      "tensor(0.4722)\n",
      "tensor(0.2386)\n",
      "tensor(0.4006)\n",
      "tensor(0.3213)\n",
      "tensor(0.3688)\n",
      "tensor(0.4777)\n",
      "tensor(0.4289)\n",
      "tensor(0.3265)\n",
      "tensor(0.5193)\n",
      "tensor(0.5770)\n",
      "tensor(0.5011)\n",
      "tensor(0.3437)\n",
      "tensor(0.2972)\n",
      "第4个EPOCH达到最低loss\n",
      "第4个Epoch的训练时间99.5589189529419s, TrainLoss:0.41207170486450195, TestLoss:0.6264330744743347\n",
      "tensor(0.2873)\n",
      "tensor(0.3648)\n",
      "tensor(0.4568)\n",
      "tensor(0.3692)\n",
      "tensor(0.2662)\n",
      "tensor(0.3949)\n",
      "tensor(0.1937)\n",
      "tensor(0.3830)\n",
      "tensor(0.4265)\n",
      "tensor(0.2469)\n",
      "tensor(0.2768)\n",
      "tensor(0.6523)\n",
      "tensor(0.2668)\n",
      "tensor(0.4590)\n",
      "tensor(0.2816)\n",
      "tensor(0.3423)\n",
      "tensor(0.2686)\n",
      "tensor(0.2882)\n",
      "tensor(0.3081)\n",
      "tensor(0.3808)\n",
      "tensor(0.3322)\n",
      "tensor(0.5689)\n",
      "tensor(0.3052)\n",
      "tensor(0.2624)\n",
      "tensor(0.4255)\n",
      "tensor(0.3319)\n",
      "tensor(0.5146)\n",
      "tensor(0.2744)\n",
      "tensor(0.3385)\n",
      "tensor(0.3203)\n",
      "tensor(0.5014)\n",
      "tensor(0.4489)\n",
      "tensor(0.3828)\n",
      "tensor(0.4122)\n",
      "tensor(0.3478)\n",
      "tensor(0.4005)\n",
      "tensor(0.3553)\n",
      "tensor(0.3399)\n",
      "tensor(0.3404)\n",
      "tensor(0.3103)\n",
      "tensor(0.1879)\n",
      "tensor(0.2757)\n",
      "tensor(0.3529)\n",
      "tensor(0.4314)\n",
      "tensor(0.5985)\n",
      "tensor(0.5864)\n",
      "tensor(0.2845)\n",
      "tensor(0.4051)\n",
      "tensor(0.4390)\n",
      "tensor(0.3533)\n",
      "tensor(0.4585)\n",
      "tensor(0.3150)\n",
      "tensor(0.2590)\n",
      "tensor(0.3672)\n",
      "tensor(0.6989)\n",
      "tensor(0.4832)\n",
      "tensor(0.3165)\n",
      "tensor(0.3324)\n",
      "tensor(0.3613)\n",
      "tensor(0.2999)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#记录一个epoch运行的时间\u001B[39;00m\n\u001B[0;32m     10\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (x,y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#每次update更新梯度\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# 获取图像\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[3], line 26\u001B[0m, in \u001B[0;36mSegmentDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq:\n\u001B[0;32m     25\u001B[0m     segmap \u001B[38;5;241m=\u001B[39m SegmentationMapsOnImage(mask,mask\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 26\u001B[0m     img,mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43msegmentation_maps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msegmap\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# 获取数组内容\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     mask \u001B[38;5;241m=\u001B[39m mask\u001B[38;5;241m.\u001B[39mget_arr()\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\meta.py:2008\u001B[0m, in \u001B[0;36mAugmenter.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2006\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   2007\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2008\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maugment(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\meta.py:1979\u001B[0m, in \u001B[0;36mAugmenter.augment\u001B[1;34m(self, return_batch, hooks, **kwargs)\u001B[0m\n\u001B[0;32m   1968\u001B[0m \u001B[38;5;66;03m# augment batch\u001B[39;00m\n\u001B[0;32m   1969\u001B[0m batch \u001B[38;5;241m=\u001B[39m UnnormalizedBatch(\n\u001B[0;32m   1970\u001B[0m     images\u001B[38;5;241m=\u001B[39mimages,\n\u001B[0;32m   1971\u001B[0m     heatmaps\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheatmaps\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1976\u001B[0m     line_strings\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mline_strings\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m   1977\u001B[0m )\n\u001B[1;32m-> 1979\u001B[0m batch_aug \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_batch_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1981\u001B[0m \u001B[38;5;66;03m# return either batch or tuple of augmentables, depending on what\u001B[39;00m\n\u001B[0;32m   1982\u001B[0m \u001B[38;5;66;03m# was requested by user\u001B[39;00m\n\u001B[0;32m   1983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_batch:\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\meta.py:641\u001B[0m, in \u001B[0;36mAugmenter.augment_batch_\u001B[1;34m(self, batch, parents, hooks)\u001B[0m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _maybe_deterministic_ctx(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batch_inaug\u001B[38;5;241m.\u001B[39mempty:\n\u001B[1;32m--> 641\u001B[0m         batch_inaug \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_augment_batch_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_inaug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparents\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mparents\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    647\u001B[0m \u001B[38;5;66;03m# revert augmentables being set to None for non-activated augmenters\u001B[39;00m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m set_to_none:\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\meta.py:3124\u001B[0m, in \u001B[0;36mSequential._augment_batch_\u001B[1;34m(self, batch, random_state, parents, hooks)\u001B[0m\n\u001B[0;32m   3121\u001B[0m         order \u001B[38;5;241m=\u001B[39m sm\u001B[38;5;241m.\u001B[39mxrange(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m))\n\u001B[0;32m   3123\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m order:\n\u001B[1;32m-> 3124\u001B[0m         batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_batch_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3125\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3126\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparents\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3127\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhooks\u001B[49m\n\u001B[0;32m   3128\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m batch\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\meta.py:641\u001B[0m, in \u001B[0;36mAugmenter.augment_batch_\u001B[1;34m(self, batch, parents, hooks)\u001B[0m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _maybe_deterministic_ctx(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batch_inaug\u001B[38;5;241m.\u001B[39mempty:\n\u001B[1;32m--> 641\u001B[0m         batch_inaug \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_augment_batch_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_inaug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparents\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mparents\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    647\u001B[0m \u001B[38;5;66;03m# revert augmentables being set to None for non-activated augmenters\u001B[39;00m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m set_to_none:\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\geometric.py:4326\u001B[0m, in \u001B[0;36mElasticTransformation._augment_batch_\u001B[1;34m(self, batch, random_state, parents, hooks)\u001B[0m\n\u001B[0;32m   4323\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_draw_samples(\u001B[38;5;28mlen\u001B[39m(shapes), random_state)\n\u001B[0;32m   4325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, shape \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(shapes):\n\u001B[1;32m-> 4326\u001B[0m     dx, dy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_shift_maps\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4327\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4328\u001B[0m \u001B[43m        \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malphas\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4329\u001B[0m \u001B[43m        \u001B[49m\u001B[43msigma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msigmas\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_states\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4332\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mimages \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4333\u001B[0m         batch\u001B[38;5;241m.\u001B[39mimages[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_augment_image_by_samples(\n\u001B[0;32m   4334\u001B[0m             batch\u001B[38;5;241m.\u001B[39mimages[i], i, samples, dx, dy)\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\geometric.py:4547\u001B[0m, in \u001B[0;36mElasticTransformation._generate_shift_maps\u001B[1;34m(cls, shape, alpha, sigma, random_state)\u001B[0m\n\u001B[0;32m   4543\u001B[0m dy_unsmoothed \u001B[38;5;241m=\u001B[39m dxdy_unsmoothed[h_pad:, :]\n\u001B[0;32m   4545\u001B[0m \u001B[38;5;66;03m# TODO could this also work with an average blur? would probably be\u001B[39;00m\n\u001B[0;32m   4546\u001B[0m \u001B[38;5;66;03m#      faster\u001B[39;00m\n\u001B[1;32m-> 4547\u001B[0m dx \u001B[38;5;241m=\u001B[39m \u001B[43mblur_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblur_gaussian_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdx_unsmoothed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m alpha\n\u001B[0;32m   4548\u001B[0m dy \u001B[38;5;241m=\u001B[39m blur_lib\u001B[38;5;241m.\u001B[39mblur_gaussian_(dy_unsmoothed, sigma) \u001B[38;5;241m*\u001B[39m alpha\n\u001B[0;32m   4550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m padding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mE:\\Anaconda3\\envs\\very\\lib\\site-packages\\imgaug\\augmenters\\blur.py:249\u001B[0m, in \u001B[0;36mblur_gaussian_\u001B[1;34m(image, sigma, ksize, backend, eps)\u001B[0m\n\u001B[0;32m    246\u001B[0m ksize \u001B[38;5;241m=\u001B[39m ksize \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ksize \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m ksize\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ksize \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 249\u001B[0m     image_warped \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGaussianBlur\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_normalize_cv2_input_arr_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43mksize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mksize\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43msigmaX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msigma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43msigmaY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msigma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mborderType\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBORDER_REFLECT_101\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;66;03m# re-add channel axis removed by cv2 if input was (H, W, 1)\u001B[39;00m\n\u001B[0;32m    257\u001B[0m     image \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    258\u001B[0m         image_warped[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, np\u001B[38;5;241m.\u001B[39mnewaxis]\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m image_warped\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m image_warped)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 200\n",
    "# 记录最小测试loss\n",
    "best_test_loss = 100\n",
    "for epoch in range(EPOCH):\n",
    "    # 获取每一批次的图像信息\n",
    "\n",
    "    # 计算整批数据的loss\n",
    "    loss = 0\n",
    "    #记录一个epoch运行的时间\n",
    "    start_time = time.time()\n",
    "    for i, (x,y) in enumerate(train_loader):\n",
    "        #每次update更新梯度\n",
    "        model.zero_grad()\n",
    "        # 获取图像\n",
    "        x = x.to(device,dtype = torch.float32)\n",
    "        # 获取标注\n",
    "        y = y.to(device,dtype = torch.float32)\n",
    "\n",
    "        # 获取预测值\n",
    "        y_pred = model(x)\n",
    "        loss_batch = loss_fn(y_pred,y)\n",
    "\n",
    "        #梯度\n",
    "        loss_batch.backward()\n",
    "\n",
    "        # 计算梯度\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 获取每个batch的训练loss\n",
    "        loss_batch = loss_batch.detach().to('cpu')\n",
    "        print(loss_batch)\n",
    "        loss += loss_batch\n",
    "\n",
    "    # 计算loss\n",
    "    loss = loss/(len(train_loader))  # 这是整个一轮的整批数据的loss\n",
    "\n",
    "    # 降低lr，如果在连续10个epoch上损失都不再下降，则降低lr\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    # 计算测试集loss\n",
    "    test_loss = check_test_loss(test_loader,model)\n",
    "\n",
    "\n",
    "    # 记录到tensorboard可视化\n",
    "    writer.add_scalar(\"LOSS/train\",loss,epoch)\n",
    "    writer.add_scalar(\"LOSS/test\",test_loss,epoch)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        # 保存模型\n",
    "        torch.save(model.state_dict(),'./saved_model/unet_best.pt')\n",
    "        print(\"第{}个EPOCH达到最低loss\".format(epoch))\n",
    "\n",
    "    print(\"第{}个Epoch的训练时间{}s, TrainLoss:{}, TestLoss:{}\".format(epoch,time.time() - start_time,loss,test_loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "161"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
